{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f891b21c-43a5-46e7-93a6-84721adc91f0",
    "_uuid": "6c2350adbb20ec2b769d7e819d89f87d62b86c59"
   },
   "source": [
    "## **Table of Contents:**\n",
    "* Introduction\n",
    "* Exploratory data analysis\n",
    "* Data Preprocessing\n",
    "    - Converting Features\n",
    "    - Creating Categories\n",
    "    - Creating new Features\n",
    "* Building Machine Learning Models\n",
    "    - Training different models\n",
    "    - Which is the best model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "14ff033f-691f-49e3-b1b2-260b28adfe58",
    "_uuid": "9d5cdd7511862e240273f2687c2d69f3c9bc8568"
   },
   "source": [
    "# **Introduction**\n",
    "\n",
    "The database we are working with classifies people into 16 distinct personality types showing their last 50 tweets, separated by \"|||\". \n",
    "\n",
    "\n",
    "MYERS BRIGGS CLASSIFICATION PROBLEM\n",
    "\n",
    "The Myers Briggs Type Indicator (or MBTI for short) is a personality type system that divides everyone into 16 distinct personality types across 4 axis:\n",
    "\n",
    "Introversion (I) – Extroversion (E)\n",
    "\n",
    "Intuition (N) – Sensing (S)\n",
    "\n",
    "Thinking (T) – Feeling (F)\n",
    "\n",
    "Judging (J) – Perceiving (P)\n",
    "\n",
    "(Note that the opposite personalities are aligned above to give one a sense of difference in the meanings of the personalities while compariing them with each other.)\n",
    "\n",
    "So for example, someone who prefers introversion, intuition, thinking and perceiving would be labelled an INTP in the MBTI system, and there are lots of personality based components that would model or describe this person’s preferences or behaviour based on the label.\n",
    "\n",
    "It is one of, if not the, the most popular personality test in the world. It is used in businesses, online, for fun, for research and lots more. A simple google search reveals all of the different ways the test has been used over time. It’s safe to say that this test is still very relevant in the world in terms of its use.\n",
    "\n",
    "From scientific or psychological perspective it is based on the work done on cognitive functions by Carl Jung i.e. Jungian Typology. This was a model of 8 distinct functions, thought processes or ways of thinking that were suggested to be present in the mind. Later this work was transformed into several different personality systems to make it more accessible, the most popular of which is of course the MBTI.\n",
    "\n",
    "Recently, its use/validity has come into question because of unreliability in experiments surrounding it, among other reasons. But it is still clung to as being a very useful tool in a lot of areas, and the purpose of this dataset is to help see if any patterns can be detected in specific types and their style of writing, which overall explores the validity of the test in analysing, predicting or categorising behaviour. Content\n",
    "\n",
    "This dataset contains over 8600 rows of data, on each row is a person’s:\n",
    "\n",
    "Type (This persons 4 letter MBTI code/type)\n",
    "A section of each of the last 50 things they have posted (Each entry separated by \"|||\" (3 pipe characters))\n",
    "![Meyer Briggs Personality Template](https://theelementsofdigitalstyle.com/wp-content/uploads/Myers-Briggs-illustration-3-2.jpg)\n",
    "\n",
    "Our goal will be to create new columns based on the content of the tweets, in order to create a predictive model. As we will see, this can be quite tricky and our creativity comes into play when analysing the content of the tweets.\n",
    "\n",
    "We begin by importing our dataset and showing some info, for an initial exploratory analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "c1984163-fffe-41aa-99a0-243673f1bf51",
    "_uuid": "e1fd66d81a1166c3f8054857e1824c5de671bb09",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import collections\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from bokeh.io import output_file, show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "b09305193d00647614c4c5e0eeda76a4e7374816"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/mbti_1.csv')\n",
    "print(df.head(10))\n",
    "print(\"*\"*40)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fb2b5c26-42e1-4b04-8ef4-16999b6ab6f1",
    "_uuid": "84dec5efb299b0d3fb4988a295e8c03087539f26"
   },
   "source": [
    "We can thus see that there are no null inputs, which means there is no need for cleaning the data. \n",
    "\n",
    "The first idea that pops up is checking if the words per tweet of each person shows us some information. For that reason, we can create a new column as shown below. This the first step for feature extraction of these high level values, so we can then do our regressional models later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "e8029602-1dc9-4a2a-b4e1-7072f81c565b",
    "_uuid": "ffcbdf4d43d2017e187f58a69d58cde88f5206b2"
   },
   "outputs": [],
   "source": [
    "df['words_per_comment'] = df['posts'].apply(lambda x: len(x.split())/50)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f2a405e1-afa0-4dbb-9ee8-1164e1be07bb",
    "_uuid": "d1a10157637263e2867e24564a19368ea371708a"
   },
   "source": [
    "# **Exploratory data analysis**\n",
    "\n",
    "We may use it for one reason or for another, but one thing we can do is printing a violin plot. \n",
    "\n",
    "At the end I did not use it at all, but it is always nice to have the ability do some visual analysis for further investigations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "29fc9dd1-39f2-49fa-8034-a887def80113",
    "_uuid": "903c6696d37f517ec51d35309e902e46daf2d789"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.violinplot(x='type', y='words_per_comment', data=df, inner=None, color='lightgray')\n",
    "sns.stripplot(x='type', y='words_per_comment', data=df, size=4, jitter=True)\n",
    "plt.ylabel('Words per comment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "08bbe211-a91c-45b3-9330-016dbecfca96",
    "_uuid": "a72478621467a2a67d0f561a220e26af50e37ea2"
   },
   "source": [
    "There's quite a lot of information there. \n",
    "\n",
    "Creating new columns showing the amount of questionmarks per comment, exclamations or other types will be useful later on, as we will see. This are the examples I came up with, but here is where creativity comes into play.\n",
    "\n",
    "We can also perform joint plots, pair plots and heat maps to explore relationship between data, just for fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "a5365240-b4f1-45b9-801f-639155108f45",
    "_uuid": "2a92888c4fae4668421f151609c47fde41764d22"
   },
   "outputs": [],
   "source": [
    "df['http_per_comment'] = df['posts'].apply(lambda x: x.count('http')/50)\n",
    "df['music_per_comment'] = df['posts'].apply(lambda x: x.count('music')/50)\n",
    "df['question_per_comment'] = df['posts'].apply(lambda x: x.count('?')/50)\n",
    "df['img_per_comment'] = df['posts'].apply(lambda x: x.count('jpg')/50)\n",
    "df['excl_per_comment'] = df['posts'].apply(lambda x: x.count('!')/50)\n",
    "df['ellipsis_per_comment'] = df['posts'].apply(lambda x: x.count('...')/50)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.jointplot(x='words_per_comment', y='ellipsis_per_comment', data=df, kind='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a14470ad-642a-4640-a64b-c6184c426f32",
    "_uuid": "edf176de9553085fdecbd60a477ed77aa6f7ceeb"
   },
   "source": [
    "So it seems there's a large correlation between words per comment ant the ellipsis the user types per comment! Based on pearson correlation factor there is a note of of 69% correlation between ellipsis_per_comments and words_per_comment. \n",
    "\n",
    "<center><h3> 69% ~ words correlating with ellipsis</h3> </center>\n",
    "\n",
    "This is an interesting first step, and we are interested in taking a further look into the data as we are on a mission to discover if we can predict the personality types of users based on there social media comments with a little help from machine learning. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2fa6ffe2f358f18a4347757907a8300239ca5f96"
   },
   "source": [
    "### Exploratory Analysis Pt. 2\n",
    "We are focusing on the correlation variables for  the different types of the personality types the Meyer Briggs outline in comparision to the words comments and ellipses per comment as well. Let's see which personality type will yield the highest correlation value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "7820e7c7-a4c8-4249-ae87-dae21a39608f",
    "_uuid": "7b86180aa6b7409252aecb45ad065fd3faa49718"
   },
   "outputs": [],
   "source": [
    "i = df['type'].unique()\n",
    "k = 0\n",
    "for m in range(0,2):\n",
    "    for n in range(0,6):\n",
    "        df_2 = df[df['type'] == i[k]]\n",
    "        sns.jointplot(x='words_per_comment', y='ellipsis_per_comment', data=df_2, kind=\"hex\")\n",
    "        plt.title(i[k])\n",
    "        plt.show()\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "dcdb7444-2da6-4660-acfe-5a9321a3af35",
    "_uuid": "831d063f8b916840403709f51736ad616aad99e8"
   },
   "outputs": [],
   "source": [
    "i = df['type'].unique()\n",
    "k = 0\n",
    "TypeArray = []\n",
    "PearArray=[]\n",
    "for m in range(0,2):\n",
    "    for n in range(0,6):\n",
    "        df_2 = df[df['type'] == i[k]]\n",
    "        pearsoncoef1=np.corrcoef(x=df_2['words_per_comment'], y=df_2['ellipsis_per_comment'])\n",
    "        pear=pearsoncoef1[1][0]\n",
    "        print(pear)\n",
    "        TypeArray.append(i[k])\n",
    "        PearArray.append(pear)\n",
    "        k+=1\n",
    "\n",
    "\n",
    "TypeArray = [x for _,x in sorted(zip(PearArray,TypeArray))]\n",
    "PearArray = sorted(PearArray, reverse=True)\n",
    "print(PearArray)\n",
    "print(TypeArray)\n",
    "plt.scatter(TypeArray, PearArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "835fb91ef4cfcd45d3b617771e4ea25a1727bbe8"
   },
   "source": [
    "# Discussion \n",
    "Highest correlation values to the ellispes and comments are as following for the top three:\n",
    "* INFJ  - Intorversion Intuition Feeling Judging \n",
    "* INTP - The Thinker - Introversion Intuion Thinking Percieving  \n",
    "* ENFP - The Inspirer - Extroverted Intuition Feeling Percieving \n",
    "\n",
    "\n",
    "View all of the different personality types and meanings [here](http://http://www.personalitypage.com/html/high-level.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "54495ba1-ad20-4149-a843-f927c40d0f56",
    "_uuid": "32a81450ae9c91a8ff26106b8e05362543d74aff"
   },
   "source": [
    "# **Data preprocessing**\n",
    "\n",
    "To get a further insight on our dataset, we can first create 4 new columns dividing the people by introversion/extroversion, intuition/sensing, and so on. \n",
    "\n",
    "When it comes to performing machine learning, trying to distinguish between two categories is much easier than distinguishing between 16 categories. We will check that later on. Dividing the data in 4 small groups will perhaps be more useful when it comes to accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "324bae84-3068-4bb6-b0ec-11a76683ddd5",
    "_uuid": "2a2c8c86d9f4cfe5bac1a34b23eb1bc71cbaacd6"
   },
   "outputs": [],
   "source": [
    "map1 = {\"I\": 0, \"E\": 1}\n",
    "map2 = {\"N\": 0, \"S\": 1}\n",
    "map3 = {\"T\": 0, \"F\": 1}\n",
    "map4 = {\"J\": 0, \"P\": 1}\n",
    "df['I-E'] = df['type'].astype(str).str[0]\n",
    "df['I-E'] = df['I-E'].map(map1)\n",
    "df['N-S'] = df['type'].astype(str).str[1]\n",
    "df['N-S'] = df['N-S'].map(map2)\n",
    "df['T-F'] = df['type'].astype(str).str[2]\n",
    "df['T-F'] = df['T-F'].map(map3)\n",
    "df['J-P'] = df['type'].astype(str).str[3]\n",
    "df['J-P'] = df['J-P'].map(map4)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6441ac53-9d10-4312-9191-0f0ecd440f2e",
    "_uuid": "1488ee7975b272f31b284e6b7e6c8580042c150d"
   },
   "source": [
    "# **Building machine learning algorithms**\n",
    "\n",
    "Let's do some machine learning now, first with the entire \"type\" column, with different models. Let's crank it out to the max! We first remove all of the columns that are not our feature to properly set up our model. In the future we need to use word2vec to get the best output from this data, but our there will be some positive results from our current data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "0acd5e9e-22e6-4f4f-afe0-76e2a54507e2",
    "_uuid": "8c0d4b61a57e057d26dc858c317bab32d9541580"
   },
   "outputs": [],
   "source": [
    "X = df.drop(['type','posts','I-E','N-S','T-F','J-P'], axis=1).values\n",
    "y = df['type'].values\n",
    "\n",
    "print(y.shape)\n",
    "print(X.shape)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3, random_state=5)\n",
    "\n",
    "sgd = SGDClassifier(n_iter=5)\n",
    "sgd.fit(X_train, y_train)\n",
    "Y_pred = sgd.predict(X_test)\n",
    "sgd.score(X_train, y_train)\n",
    "acc_sgd = round(sgd.score(X_train, y_train) * 100, 2)\n",
    "print(round(acc_sgd,2,), \"%\")\n",
    "acc_sgd = round(sgd.score(X_test, y_test) * 100, 2)\n",
    "print(round(acc_sgd,2,), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "75aa73c4-8331-41c2-b310-2e7f423611c7",
    "_uuid": "30168f35c770296823db30dfedad1146ee8042b7"
   },
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "Y_prediction = random_forest.predict(X_test)\n",
    "\n",
    "random_forest.score(X_train, y_train)\n",
    "acc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\n",
    "print(\"Training Data Set\",round(acc_random_forest,2,), \"%\")\n",
    "\n",
    "acc_random_forest = round(random_forest.score(X_test, y_test) * 100, 2)\n",
    "print(\"Testing Data Set\", round(acc_random_forest,2,), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "97b22da6-7fdd-4abb-8df6-f11deba405a1",
    "_uuid": "4a28af6cbb1041fe234a6641d49c00d60fe9f5f0"
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "Y_pred = logreg.predict(X_test)\n",
    "\n",
    "acc_log = round(logreg.score(X_train, y_train) * 100, 2)\n",
    "print(\"Training Data Set\",round(acc_log,2,), \"%\")\n",
    "\n",
    "acc_log = round(logreg.score(X_test, y_test) * 100, 2)\n",
    "print(\"Testing Data Set\",round(acc_log,2,), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "8d26f82f-2621-4f24-883e-8ea067759f94",
    "_uuid": "015d0834932cb68d9f4062059fe97b3d0538cf28",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KNN\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "Y_pred = knn.predict(X_test)\n",
    "\n",
    "acc_knn = round(knn.score(X_train, y_train) * 100, 2)\n",
    "print(round(acc_knn,2,), \"%\")\n",
    "\n",
    "acc_knn = round(knn.score(X_test, y_test) * 100, 2)\n",
    "print(round(acc_knn,2,), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "62321bef8492eef36b529c5fc7d6240836f4c13d"
   },
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "e118ba3b885d55fb74c8ca3f34cd52510f939164"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "f73652182fa77ae5929ebbdf9c5aff7b40aff2ed",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting the output to int from 0 to 15\n",
    "y = pd.factorize(df['type'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "f36d5db8086a5a70ea777a714117479c8afc4abe",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Splitting the data into valid for crossvalidation\n",
    "X = df.drop(['type','posts','I-E','N-S','T-F','J-P'], axis=1).values\n",
    "y = pd.factorize(df['type'])[0] \n",
    "# Splitting 70 ,20 ,10 to train, test , valid\n",
    "# Valid data for cross validation\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=1)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train,y_train, test_size=0.15, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "4afe3e94e56cfba77f7f9ddc518365f3a5107630",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert the output to binary 0 and 1\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_valid = to_categorical(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "65052384a50054710733cf315648ac67c6ac1f4e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32,input_dim=x_train.shape[1])) #32 can be change to any number\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Dense(32)) #2**any_number\n",
    "model.add(Dense(16)) #The final dense that must be 16 for number of classes\n",
    "model.compile(loss = 'categorical_crossentropy',# for multi class classification\n",
    "              optimizer='rmsprop',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "60086e4c87d8734bdba8731e0c6de708245263cd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each 1 epoch , it will take  100 samples\n",
    "# For 1 epoch, the model has to train on the whole train data\n",
    "model.fit(x_train,y_train,\n",
    "          validation_data=[x_valid, y_valid],# Specify the cross validation dataset\n",
    "          epochs=100,\n",
    "          batch_size=50, \n",
    "          verbose=0,\n",
    "          callbacks=[ModelCheckpoint('best_DNN_model.h5')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ff352177c1dae495ed10805263f8249166ba98d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('best_DNN_model.h5')\n",
    "DNN_test_acc = model.evaluate(x_test,y_test)[1]\n",
    "print(\"Deep Neural Network accuracy on the test data is : \" + str(DNN_test_acc))\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "77b27b3f-1669-47c8-9df9-756f79bc96a9",
    "_uuid": "68ede73124edd6cfa80fe215061c8d74ac125ed8"
   },
   "source": [
    "## Machine Learning Discussion\n",
    "Our simple model is only able to classify people with a 23% of right guesses, which is not too much. \n",
    "\n",
    "Now we will perform machine learning with the introverted/extroverted column, and we'll see if our model is able to classify if someone is introverted or extroverted with a higher precision. So let's take a deeper dive into our model to get a better perspective on our predicting. Because this isn't going to be high enough quality to be helpful for anyone. :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5fc456c1-0cb5-4a21-94ff-1b124e3198a6",
    "_uuid": "4ba8140f5e08778e0ab69fa000459a171c068d76",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XX = df.drop(['type','posts','I-E','N-S','T-F','J-P'], axis=1)\n",
    "yy = df['I-E'].values\n",
    "\n",
    "print(\"outcome shape\",yy.shape)\n",
    "print(\"input shape for machine learning data\",XX.shape)\n",
    "\n",
    "XX_train,XX_test,yy_train,yy_test=train_test_split(XX,yy,test_size = 0.2, random_state=5)\n",
    "\n",
    "sgdd = SGDClassifier(n_iter=5)\n",
    "sgdd.fit(XX_train, yy_train)\n",
    "Y_predd = sgdd.predict(XX_test)\n",
    "sgdd.score(XX_train, yy_train)\n",
    "acc_sgdd = round(sgdd.score(XX_train, yy_train) * 100, 2)\n",
    "print(round(acc_sgdd,2,), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4a3c291d-f595-4960-9f5e-9a540d8b98a2",
    "_uuid": "a5dc51c11b33647a4eaa5fb23a50fe3068d2715a",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_forestt = RandomForestClassifier(n_estimators=100)\n",
    "random_forestt.fit(XX_train, yy_train)\n",
    "\n",
    "Y_predictionn = random_forestt.predict(XX_test)\n",
    "\n",
    "random_forestt.score(XX_train, yy_train)\n",
    "acc_random_forestt = round(random_forestt.score(XX_train, yy_train) * 100, 2)\n",
    "print(\"Random Forest Predictions Model\",round(acc_random_forestt,2,), \"%\")\n",
    "\n",
    "acc_random_forestt = round(random_forestt.score(XX_test, yy_test) * 100, 2)\n",
    "print(\"Random Forest Predictions Model\",round(acc_random_forestt,2,), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_cell_guid": "d2d3c376-cc90-468b-9f5d-65a5262dddb7",
    "_uuid": "d016e4899884bd7df54f8a008ef16147828fcbae"
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "logregg = LogisticRegression()\n",
    "logregg.fit(XX_train, yy_train)\n",
    "\n",
    "Y_predd = logregg.predict(XX_test)\n",
    "\n",
    "acc_logg = round(logregg.score(XX_train, yy_train) * 100, 2)\n",
    "print(\"Logisitic Regression Prediction Accuracy\",round(acc_logg,2,), \"%\")\n",
    "\n",
    "acc_logg = round(logregg.score(XX_test, yy_test) * 100, 2)\n",
    "print(\"Logisitic Test Regression Prediction Accuracy\",round(acc_logg,2,), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "bf4145c0-54d1-4706-b436-d38689791caf",
    "_uuid": "adf9af8db8ec105cc00273b0b494fc0f70d0f811"
   },
   "outputs": [],
   "source": [
    "# KNN\n",
    "knnn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knnn.fit(XX_train, yy_train)\n",
    "\n",
    "Y_predd = knnn.predict(XX_test)\n",
    "\n",
    "acc_knnn = round(knnn.score(XX_train, yy_train) * 100, 2)\n",
    "print(\"Knn neighbor prediction value\",round(acc_knnn,2,), \"%\")\n",
    "\n",
    "acc_knnn = round(knnn.score(XX_test, yy_test) * 100, 2)\n",
    "print(\"Knn neighbor test prediction value\",round(acc_knnn,2,), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9375705f-79f3-4cb1-8d5a-16db200da250",
    "_uuid": "4fc8d8f3cef8422c708cd10da7b38762ab729c01"
   },
   "source": [
    "# Random Forest Prediction\n",
    "\n",
    "Random Forest Prediction - 100 %\n",
    "Logistic Regression Prediction - 77.1 %\n",
    "Knn neighbor prediction model - 83.66 %\n",
    "\n",
    "So we see our model has an accuracy of 77%, not bad for such a simple model! Let's see what else we can do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "f5feb407-e16a-4bfd-ab8c-90426336d3b8",
    "_uuid": "780fd3061b547a9c370c978648fde337bf47b332",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_column=[]\n",
    "for z in range(len(df['posts'])):\n",
    "    prov=df['posts'][z]\n",
    "    prov2= re.sub(r'[“€â.|,?!)(1234567890:/-]', '', prov)\n",
    "    prov3 = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', prov)\n",
    "    prov4 = re.sub(r'[|||)(?.,:1234567890!]',' ',prov3)\n",
    "    prov5 = re.sub(' +',' ', prov4)\n",
    "    prov6 = prov5.split(\" \")\n",
    "    counter = Counter(prov6)\n",
    "    counter2 = counter.most_common(1)[0][0]\n",
    "    new_column.append(counter2)\n",
    "df['most_used_word'] = new_column\n",
    "print(df.head())\n",
    "print(df['most_used_word'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "56d0ec682b4d393e32f9a328ed57fa108c1d65bb"
   },
   "source": [
    "Conclusion\n",
    "\n",
    "We have identified and outline the key personality traits of different types of people. The biggest takeaways is that personality type does not predict success or future results. What it does predict is your strengths and weaknesses in terms of personality. And one of the things I have taken into consideration is that when we are creating teams to fight crime, develop amazing software, or simple play sports. It's important to consider everyone technical position on the team, but it's also deeply important to explore to soft skills to ensure strengths and weakness are balanced. Which allows another layer of interdisciplinary not only in technicall skill set, but also in mindset and personality. In short, this is just the first step into creating a personality type model based on meyers briggs assessment from social media comment data. The next step is to focus in one close domain use cases to solve real problems. \n",
    "![Personality](https://bj1oh303t6x351kzp35xea4o-wpengine.netdna-ssl.com/wp-content/uploads/2014/08/mbti-dating-infographic-section1.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fc6497098f378400f5b048a5b2e910a03ebddbe0",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
