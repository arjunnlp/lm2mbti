{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re2 as re\n",
    "import collections\n",
    "from collections import Counter\n",
    "from bokeh.io import output_file, show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from matplotlib.pyplot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/mbti_1.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_row(row):\n",
    "    l = []\n",
    "    for i in row.split('|||'):\n",
    "        l.append(len(i.split()))\n",
    "    return np.var(l)\n",
    "\n",
    "df['words_per_comment'] = df['posts'].apply(lambda x: len(x.split())/50)\n",
    "df['variance_of_word_counts'] = df['posts'].apply(lambda x: var_row(x))\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.swarmplot(\"type\", \"words_per_comment\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,20))\n",
    "plt.xticks(fontsize=24, rotation=0)\n",
    "plt.yticks(fontsize=24, rotation=0)\n",
    "sns.countplot(data=data, x='type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Encode each type to an int\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "unique_type_list = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
    "       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n",
    "lab_encoder = LabelEncoder().fit(unique_type_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "##### Compute list of subject with Type | list of comments \n",
    "\n",
    "# Time\n",
    "%time data.posts[1].replace('+', ' ').replace('.', ' ').replace(',', ' ').replace(':', ' ')\n",
    "%time re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', data.posts[1])\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Lemmatizer | Stemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "# Cache the stop words for speed \n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "# One post\n",
    "OnePost = data.posts[1]\n",
    "\n",
    "# List all urls\n",
    "urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', OnePost)\n",
    "\n",
    "# Remove urls\n",
    "temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'link', OnePost)\n",
    "\n",
    "# Keep only words\n",
    "temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n",
    "\n",
    "# Remove spaces > 1\n",
    "temp = re.sub(' +', ' ', temp).lower()\n",
    "\n",
    "# Remove stopwords and lematize\n",
    "%time stemmer.stem(\" \".join([w for w in temp.split(' ') if w not in cachedStopWords]))\n",
    "\n",
    "print(\"\\nBefore preprocessing:\\n\\n\", OnePost[0:500])\n",
    "print(\"\\nAfter preprocessing:\\n\\n\", temp[0:500])\n",
    "print(\"\\nList of urls:\")\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Compute list of subject with Type | list of comments \n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Lemmatize\n",
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "def pre_process_data(data, remove_stop_words=True):\n",
    "\n",
    "    list_personality = []\n",
    "    list_posts = []\n",
    "    len_data = len(data)\n",
    "    i=0\n",
    "    \n",
    "    for row in data.iterrows():\n",
    "        i+=1\n",
    "        if i % 500 == 0:\n",
    "            print(\"%s | %s rows\" % (i, len_data))\n",
    "\n",
    "        ##### Remove and clean comments\n",
    "        posts = row[1].posts\n",
    "        temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'link', posts)\n",
    "        temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n",
    "        temp = re.sub(' +', ' ', temp).lower()\n",
    "        if remove_stop_words:\n",
    "            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in cachedStopWords])\n",
    "        else:\n",
    "            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ')])\n",
    "\n",
    "        type_labelized = lab_encoder.transform([row[1].type])[0]\n",
    "        list_personality.append(type_labelized)\n",
    "        list_posts.append(temp)\n",
    "\n",
    "    #del data\n",
    "    list_posts = np.array(list_posts)\n",
    "    list_personality = np.array(list_personality)\n",
    "    return list_posts, list_personality\n",
    "\n",
    "list_posts, list_personality = pre_process_data(data, remove_stop_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "cntizer = CountVectorizer(analyzer=\"word\", \n",
    "                             max_features=1500, \n",
    "                             tokenizer=None,    \n",
    "                             preprocessor=None, \n",
    "                             stop_words=None,  \n",
    "#                             ngram_range=(1,1),\n",
    "                             max_df=0.5,\n",
    "                             min_df=0.1) \n",
    "                                 \n",
    "tfizer = TfidfTransformer()\n",
    "\n",
    "print(\"CountVectorizer\")\n",
    "X_cnt = cntizer.fit_transform(list_posts)\n",
    "print(\"Tf-idf\")\n",
    "X_tfidf =  tfizer.fit_transform(X_cnt).toarray()\n",
    "reverse_dic = {}\n",
    "for key in cntizer.vocabulary_:\n",
    "    reverse_dic[cntizer.vocabulary_[key]] = key\n",
    "top_50 = np.asarray(np.argsort(np.sum(X_cnt, axis=0))[0,-50:][0, ::-1]).flatten()\n",
    "[reverse_dic[v] for v in top_50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "# Truncated SVD\n",
    "svd = TruncatedSVD(n_components=12, n_iter=7, random_state=42)\n",
    "svd_vec = svd.fit_transform(X_tfidf)\n",
    "\n",
    "print(\"TSNE\")\n",
    "X_tsne = TSNE(n_components=3, verbose=1, perplexity=40, n_iter=650).fit_transform(svd_vec)\n",
    "col = list_personality\n",
    "plt.figure(0)\n",
    "plt.scatter(X_tsne[:,0], X_tsne[:,1], c=col, cmap=plt.get_cmap('tab20') , s=12)\n",
    "plt.figure(1)\n",
    "plt.scatter(X_tsne[:,0], X_tsne[:,2], c=col, cmap=plt.get_cmap('tab20') , s=12)\n",
    "plt.figure(2)\n",
    "plt.scatter(X_tsne[:,1], X_tsne[:,2], c=col, cmap=plt.get_cmap('tab20') , s=12)\n",
    "legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA, FastICA, PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# PCA\n",
    "pca_vec = PCA(n_components=12).fit_transform(X_tfidf)\n",
    "\n",
    "# ICA\n",
    "ica_vec = FastICA(n_components=12).fit_transform(X_tfidf)\n",
    "\n",
    "# Plot\n",
    "plt.figure(1)\n",
    "plt.scatter(pca_vec[:,0], pca_vec[:,1], c=list_personality, cmap=plt.get_cmap('tab20'), s=12)\n",
    "plt.figure(2)\n",
    "plt.scatter(svd_vec[:,0], svd_vec[:,1], c=list_personality, cmap=plt.get_cmap('tab20'), s=12)\n",
    "plt.figure(3)\n",
    "plt.scatter(ica_vec[:,0], ica_vec[:,1], c=list_personality, cmap=plt.get_cmap('tab20'), s=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split mbti personality into 4 letters and binarize\n",
    "titles = [\"Extraversion (E) - Introversion (I)\",\n",
    "          \"Sensation (S) - INtuition (N)\",\n",
    "          \"Thinking (T) - Feeling (F)\",\n",
    "          \"Judgement (J) - Perception (P)\"\n",
    "         ] \n",
    "b_Pers = {'I':0, 'E':1, 'N':0, 'S':1, 'F':0, 'T':1, 'J':0, 'P':1}\n",
    "b_Pers_list = [{0:'I', 1:'E'}, {0:'N', 1:'S'}, {0:'F', 1:'T'}, {0:'J', 1:'P'}]\n",
    "\n",
    "def translate_personality(personality):\n",
    "    '''\n",
    "    transform mbti to binary vector\n",
    "    '''\n",
    "    return [b_Pers[l] for l in personality]\n",
    "\n",
    "def translate_back(personality):\n",
    "    '''\n",
    "    transform binary vector to mbti personality\n",
    "    '''\n",
    "    s = \"\"\n",
    "    for i, l in enumerate(personality):\n",
    "        s += b_Pers_list[i][l]\n",
    "    return s\n",
    "\n",
    "list_personality_bin = np.array([translate_personality(p) for p in data.type])\n",
    "print(\"Binarize MBTI list: \\n%s\" % list_personality_bin)\n",
    "\n",
    "# Plot\n",
    "def plot_tsne(X, i):\n",
    "    a = plt.figure(i, figsize=(30,20))\n",
    "    plt.title(titles[i])\n",
    "    plt.subplot(3,1,1)\n",
    "    plt.scatter(X[:,0], X[:,1], c=list_personality_bin[:,i], cmap=plt.get_cmap('Dark2'), s=25)\n",
    "    plt.subplot(3,1,2)\n",
    "    plt.scatter(X[:,0], X[:,2], c=list_personality_bin[:,i], cmap=plt.get_cmap('Dark2'), s=25)\n",
    "    plt.subplot(3,1,3)\n",
    "    plt.scatter(X[:,1], X[:,2], c=list_personality_bin[:,i], cmap=plt.get_cmap('Dark2'), s=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(X_tsne, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(X_tsne, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(X_tsne, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(X_tsne, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
